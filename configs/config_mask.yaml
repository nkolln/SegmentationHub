project_name: "paintable-surface-segmentation"
experiment_name: "mask2formert4_new_head_clamp_kfold"

model:
  name: "mask2former" # Options: unet_viable, segformer_hf, mask2former, dinov2
  num_classes: 4
  pretrained: true
  encoder_name: "swin-tiny"  # Backbone variant
  # pretrained_repo: "facebook/mask2former-swin-large-cityscapes-semantic"
  pretrained_repo: "facebook/mask2former-swin-tiny-cityscapes-semantic"

training:
  batch_size: 2 # Mask2Former is memory intensive
  gradient_accumulation_steps: 4 # Effective Batch Size = 8
  num_epochs: 80 
  learning_rate: 0.0001
  max_lr: 0.0005
  weight_decay: 0.05
  optimizer: "adamw"
  scheduler: "onecycle"
  warmup_ratio: 0.1
  seed: 42
  use_amp: true # Transformer-based models benefit greatly from AMP
  early_stopping_patience: 15
  # Encoder freezing options
  freeze_encoder: true           
  unfreeze_epoch: 8
  encoder_lr_mult: 0.3
  use_tta: false # Keep TTA enabled for best metrics

loss:
  cross_entropy: 0.5 # Reverted to match old run defaults
  dice: 0.5          # Reverted to match old run defaults
  focal:
    weight: 0.0      # Disabled to match old run
    alpha: 0.25
    gamma: 2.0
  boundary: 0.0      # Disabled to match old run
  label_smoothing: 0.1
  ignore_index: 255

data:
  root_dir: "data"
  sources: ["extended"]
  image_size: 512
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1
  # k_folds: 5
  num_workers: 0
  class_mapping:
    0: 0
    1: 1  # Background
    2: 2  # Facade
    3: 3  # Window
    4: 0
    5: 0
    6: 0
    7: 0
    8: 3
    9: 0 
    10: 0 
    11: 0 
    12: 0

logging:
  use_wandb: true
  log_every_n_steps: 10
  save_checkpoint_every_n_epochs: 5
  output_dir: "outputs/mask2former"

monitoring:
  track_gradients: true
  gradient_log_frequency: 10
  visualize_predictions_every_n_epochs: 5
